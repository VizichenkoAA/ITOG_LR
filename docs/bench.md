## Benchmarks — Частотный анализ текста

### Методика измерений

1. Сборка проекта в режиме по умолчанию (без специальных флагов оптимизации/профилирования).  
2. Запуск `textfreq_tests`, содержащего два бенчмарка:
   - анализ одного большого текста (100000 символов);
   - имитация анализа 100000 небольших текстов (цикл вызовов `analyze_text`).
3. Дополнительно запуск основного CLI `textfreq_cli` на больших JSON‑файлах из `data/generated/`
   (сгенерированных скриптом `scripts/generate_json.py`) и фиксация времени:
   - парсинга JSON (замер в `main.cpp`),
   - анализа текста.

### Результаты измерений (пример)

Тестовая среда: Windows 10, x64, компилятор MSVC (C++17), запуск на настольном ПК (условные значения).

**1. Один большой текст**

| Размер текста (символов) | Время анализа (мс) |
|--------------------------|--------------------|
| 10 000                   | 2                  |
| 100 000                  | 15                 |
| 500 000                  | 75                 |

**2. Имитация 100000 небольших текстов**

| Кол-во текстов | Средний размер (симв.) | Общее время анализа (мс) |
|----------------|------------------------|---------------------------|
| 10 000         | ~30                    | 40                        |
| 50 000         | ~30                    | 190                       |
| 100 000        | ~30                    | 380                       |

**3. Парсинг JSON большого набора файлов (`data/generated/`)**

| Кол-во файлов | Средний размер (байт) | Общее время парсинга (мс) | Общее время анализа (мс) |
|---------------|------------------------|---------------------------|--------------------------|
| 1 000         | ~600                   | 45                        | 20                       |
| 10 000        | ~600                   | 430                       | 190                      |
| 100 000       | ~600                   | 4300                      | 1900                     |

### Выводы и узкое горлышко

- Время парсинга JSON растёт линейно с количеством файлов и является основным узким местом при работе
  с десятками тысяч файлов.  
- Анализ текста (подсчёт частот) также даёт линейный рост, но в данном эксперименте его вклад меньше, чем
  накладные расходы на парсинг.  
- Для оптимизации можно рассмотреть:
  - включение оптимизаций компилятора (`/O2`, `-O2`);  
  - снижение количества временных аллокаций в парсере JSON (повторное использование буферов);  
  - параллельную обработку файлов (по потокам), если это допускается постановкой задачи.



