## Отчёт по итоговой лабораторной работе (Частотный анализ текста)

1. **Титульный лист**
   - ФИО: *Вижиченко Александр Алексеевич*  
   - Группа: **  
   - Тема: Частотный анализ текста в пользовательском JSON.

2. **Краткое описание задачи и среды выполнения**

   Задача: разработать консольное приложение на C++, которое загружает текст и список стоп‑слов из JSON‑файлов,
   выполняет частотный анализ (количество слов, предложений, уникальных слов, топ по частоте, распределение по длине)
   и формирует человекочитаемый отчёт. Дополнительно требуется провести замеры производительности и проанализировать
   узкое место при обработке больших объёмов данных.

   Среда выполнения (пример):
   - ОС: Windows 10 x64  
   - Компилятор: MSVC (C++17) / MinGW‑g++ (C++17)  
   - Сборка: CMake 3.16+  
   - IDE/редактор: Visual Studio Code / Cursor.

3. **Структура проекта и схема данных**

   Структура каталогов:
   - `src/` — исходный код программы (`main.cpp`, `cli.cpp`, `json_parser.cpp`, `text_analyzer.cpp`);  
   - `include/` — заголовочные файлы с интерфейсами (`cli.hpp`, `json_parser.hpp`, `text_analyzer.hpp`);  
   - `tests/` — самотесты и бенчмарки (`tests_main.cpp`);  
   - `data/` — примеры JSON‑файлов (минимальные, с массивом параграфов, ошибочные, сгенерированные);  
   - `docs/` — план реализации (`Implementation_Plan.md`), отчёты (`bench.md`, `report.md`);  
   - `scripts/` — вспомогательные скрипты (генерация до 100000 JSON‑файлов).

   Мини‑спецификация JSON:
   - Входной текст:
     - либо объект: `{ "text": "строка с полным текстом" }`,  
     - либо массив параграфов: `[ { "paragraph": "текст абзаца 1" }, { "paragraph": "..." } ]`.  
   - Стоп‑слова:
     - либо массив строк: `[ "и", "в", "на", ... ]`,  
     - либо массив объектов: `[ { "stop": "и" }, { "stop": "в" }, ... ]`.

4. **Описание ключевых алгоритмов**

   - **Парсер JSON**: реализован в `json::Parser` (ручной парсинг минимального подмножества JSON: объекты, массивы,
     строки, числа, литералы `true/false/null`; обрабатываются экранирования `\"`, `\\`, `\n` и др.). При ошибке
     выбрасывается исключение `ParseError` с позицией символа.
   - **Разбор текста**: текст нормализуется к нижнему регистру, слова выделяются по буквенно‑цифровым символам и апострофу,
     предложения считаются по символам `.`, `!`, `?`.  
   - **Частотный анализ**: для каждого слова считается частота (все слова), отдельно частота без стоп‑слов, а также
     распределение по длине слов. Результаты агрегируются в структуре `TextStats`.

5. **Интерфейсы функций и примеры использования**

   Основные интерфейсы:
   - `json::Value json::Parser::parse()` — разобрать строку JSON в дерево значений;  
   - `std::vector<std::string> extract_text_blocks(const json::Value&)` — извлечь список текстовых блоков;  
   - `std::vector<std::string> extract_stopwords(const json::Value&)` — извлечь список стоп‑слов;  
   - `TextStats analyze_text(const std::vector<std::string>&, const std::vector<std::string>&)` — выполнить анализ;  
   - `std::string format_report(const TextStats&, size_t top_n)` — сформировать отчёт;  
   - `CliOptions parse_arguments(int argc, char** argv)` — разобрать аргументы командной строки.

   Пример запуска CLI:

   ```bash
   textfreq_cli --input data/sample_text.json \
                --stops data/stopwords.json \
                --report freq --top 20
   ```

   Пример с выводом в файл:

   ```bash
   textfreq_cli --input data/paragraphs_array.json \
                --stops data/stopwords.json \
                --report freq --top 50 \
                --output data/report.txt
   ```

6. **Обработка ошибок и UX взаимодействия**

   - При невозможности открыть файл ввода/вывода пользователь получает сообщение с указанием пути файла.  
   - При ошибке парсинга JSON выводится позиция и краткое описание проблемы (например, пропущенная запятая).  
   - При отсутствии ожидаемого ключа (`"text"` или массива параграфов) программа сообщает, что текст не найден.  
   - Стоп‑слова обрабатываются мягко: если их файл отсутствует или содержит некорректные структуры, анализ выполняется
     только по словам без фильтрации, что явно указывается в сообщении.  
   - В отчёте выводятся читаемые таблицы с выравниванием колонок; `--help` описывает все флаги и приводит примеры.

7. **Результаты тестов**

   - Позитивные тесты:
     - корректный файл `{ "text": "Hello world. Hello C++!" }` даёт ожидаемое количество слов и предложений;  
     - массив параграфов в `data/paragraphs_array.json` корректно объединяется в список блоков;  
     - список стоп‑слов в формате массива строк и в формате `{ "stop": "..." }` обрабатывается одинаково.
   - Негативные тесты:
     - `data/invalid_broken_json.json` — проверка обработки синтаксической ошибки JSON;  
     - `data/invalid_missing_text.json` — корректный JSON без ключа `"text"`;  
     - `data/invalid_wrong_type.json` — поле `"text"` неверного типа (число вместо строки).  
   - Граничные случаи:
     - пустой текст, текст только из пунктуации, текст, состоящий только из стоп‑слов.

   Результаты самотестов и бенчмарков выводятся при запуске `textfreq_tests` и могут быть сохранены в файл (скриншоты/логи —
   в каталоге `docs/`).

8. **Результаты бенчмарков**

   Подробные таблицы с экспериментальными данными приведены в `docs/bench.md`. В кратком виде:
   - время анализа одного большого текста растёт линейно с размером (на уровне десятков миллисекунд для сотен тысяч символов);  
   - имитация анализа 100000 небольших текстов показывает почти линейную зависимость от количества итераций;  
   - при обработке большого количества файлов основным узким местом становится парсинг JSON, а не сам частотный анализ.

9. **Выводы и возможные улучшения**

   В ходе работы было реализовано консольное C++‑приложение с ручным парсером JSON, устойчивой обработкой ошибок и
   удобным текстовым интерфейсом для частотного анализа текста. Результаты тестов и бенчмарков подтверждают корректность
   работы и линейный рост времени выполнения с увеличением объёма данных.

   Возможные направления улучшения:
   - оптимизация парсера JSON (уменьшение копирований, буферизация, использование более эффективных структур данных);  
   - параллельная обработка файлов и/или параграфов для ускорения анализа на многоядерных системах;  
   - поддержка расширенного подмножества JSON и дополнительных типов отчётов (n‑граммы, биграммы, графики по времени).



